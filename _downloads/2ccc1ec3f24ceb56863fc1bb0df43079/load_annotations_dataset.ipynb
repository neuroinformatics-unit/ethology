{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load bounding box COCO annotations into ``ethology``\n\nLoad bounding box annotations in [COCO format](https://cocodataset.org/#format-data)\nas an ``ethology`` dataset and inspect it using ``ethology`` and\n[movement](https://movement.neuroinformatics.dev/) utilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pooch\nimport xarray as xr\nfrom movement.io import save_bboxes\nfrom movement.plots import plot_occupancy\nfrom movement.roi import PolygonOfInterest\n\nfrom ethology.io.annotations import load_bboxes\n\n# For interactive plots: install ipympl with `pip install ipympl` and uncomment\n# the following line in your notebook\n# %matplotlib widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download dataset\n\nFor this example, we will use the dataset from the\n[UAS Imagery of Migratory Waterfowl at New Mexico Wildlife Refuges](https://lila.science/datasets/uas-imagery-of-migratory-waterfowl-at-new-mexico-wildlife-refuges/).\nThis dataset is part of the [Drones For Ducks project](https://aspire.unm.edu/research/funded-research/ducks-and-drones.html)\nthat aims to develop an efficient method to count and identify species of\nmigratory waterfowl at wildlife refuges across New Mexico.\n\nThe dataset is made up of a set of drone images and corresponding\nbounding box annotations. Annotations are provided by both expert\nannotators and volunteers.\n\nSince the dataset is not very large, we can download it as a zip file\ndirectly from the URL provided in the dataset webpage.\nWe use the [pooch](https://github.com/fatiando/pooch/) library\nto download it to the ``.ethology`` cache directory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Source of the dataset\ndata_source = {\n    \"url\": \"https://storage.googleapis.com/public-datasets-lila/uas-imagery-of-migratory-waterfowl/uas-imagery-of-migratory-waterfowl.20240220.zip\",\n    \"hash\": \"c5b8dfc5a87ef625770ac8f22335dc9eb8a67688b610490a029dae81815a9896\",\n}\n\n# Define cache directory\nethology_cache = Path.home() / \".ethology\"\nethology_cache.mkdir(exist_ok=True)\n\n# Download the dataset to the cache directory\nextracted_files = pooch.retrieve(\n    url=data_source[\"url\"],\n    known_hash=data_source[\"hash\"],\n    fname=\"waterfowl_dataset.zip\",\n    path=ethology_cache,\n    processor=pooch.Unzip(extract_dir=ethology_cache),\n)\n\ndata_dir = ethology_cache / \"uas-imagery-of-migratory-waterfowl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this example, we will focus on the annotations labelled by the experts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "annotations_file = (\n    data_dir / \"experts\" / \"20230331_dronesforducks_expert_refined.json\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load annotations as an ``ethology`` dataset\n\nWe can use the :func:`ethology.io.annotations.load_bboxes.from_files`\nfunction to load the COCO file with the\nexpert annotations as an ``ethology`` dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds = load_bboxes.from_files(annotations_file, format=\"COCO\")\n\nprint(ds)\nprint(ds.sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the expert annotations consist of 2D bounding boxes,\ndefined for 12 images, with each image having a maximum of 722 annotations.\nThe ``position`` and ``shape`` arrays are padded with ``NaN`` for the images\nin which there are less annotations than the maximum.\nThe ``category`` array is padded with ``-1``.\n\nNote that\nin this case a single annotation ID does not represent the same\nindividual across images; it just represents an arbitrary ID assigned to\neach bounding box per image.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see from the dataset description that it includes\nfive attributes. These are stored under the ``attrs`` dictionary.\nWe can inspect the content of the ``attrs`` dictionary as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(*ds.attrs.items(), sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The attributes for the loaded dataset include two maps,\none from category IDs to category names, and one from image IDs to image\nfilenames. To inspect their values further we can use the convenient\ndot syntax:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Categories:\")\nprint(*ds.map_category_to_str.items(), sep=\"\\n\")\nprint(\"--------------------------------\")\nprint(\"Image filenames:\")\nprint(*ds.map_image_id_to_filename.items(), sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The category IDs are assigned to category names following the definition\nin the input COCO file. Usually the 0 category is reserved for the\n\"background\" class. The image IDs in the dataset are assigned based on the\nalphabetically sorted list of unique image filenames in the input file.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This dot syntax can be used to access any of the dataset attributes.\nFor example, the annotation file that was used to load the dataset can\nbe retrieved as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(ds.annotation_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise annotations\n\nLet's inspect how the annotations are distributed across the image\ncoordinate system. We can color the centroid of each bounding box by\nthe corresponding category to get a better sense of the distribution\nby species.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We use a colormap of 10 discrete colours\n# (we have 9 categories)\ncmap = plt.cm.tab10\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\n# Plot the centroids of the bounding boxes\nsc = ax.scatter(\n    ds.position.sel(space=\"x\").values,\n    ds.position.sel(space=\"y\").values,\n    s=3,\n    c=ds.category.values,\n    cmap=cmap,\n)\n\n# Add legend\n# Note: we use ds.category.values rather than\n# ds.map_category_to_str.values() because\n# the array contains the padding value -1,\n# which is also included in the scatter plot data.\nlegend_elements = [\n    plt.Line2D([0], [0], color=cmap(i)) for i in np.unique(ds.category.values)\n]\nplt.legend(\n    legend_elements,\n    ds.map_category_to_str.values(),\n    bbox_to_anchor=(1, 1),\n    loc=\"best\",\n)\nax.set_title(\"Annotations per category\")\nax.set_xlabel(\"x (pixels)\")\nax.set_ylabel(\"y (pixels)\")\nax.axis(\"equal\")\nax.invert_yaxis()\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Count annotations within a region of interest\nWe may want to compute the number of annotations within a specific region of\nthe image. We can do this using\n[movement](https://movement.neuroinformatics.dev/)\nto define a :class:`movement.roi.PolygonOfInterest` and then\ncount how many annotations are within the polygon.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define a polygon\ncentral_region = PolygonOfInterest(\n    ((1000, 500), (1000, 3000), (4500, 3000), (4500, 500)),\n    name=\"Central region\",\n)\n\n# Plot all annotations\nfig, ax = plt.subplots()\nsc = ax.scatter(\n    ds.position.sel(space=\"x\").values,\n    ds.position.sel(space=\"y\").values,\n    s=3,\n    c=ds.category.values,\n    cmap=cmap,\n)\nax.set_title(\"Annotations in polygon\")\nax.set_xlabel(\"x (pixels)\")\nax.set_ylabel(\"y (pixels)\")\nax.axis(\"equal\")\nax.invert_yaxis()\n\n\n# Plot ROI (region of interest) polygon on top\ncentral_region.plot(ax, facecolor=\"red\", edgecolor=\"red\", alpha=0.25)\n\n# Check the number of annotations in the polygon\n# Note: if position is NaN, ``.contains_point`` returns ``False``\nds_in_region = central_region.contains_point(\n    ds.position\n)  # shape: (n_images, n_max_annotations_per_image)\n\nn_annotations_in_region = ds_in_region.sum()\nn_annotations_total = (~ds.position.isnull().any(axis=1)).sum()\nfraction_in_region = n_annotations_in_region / n_annotations_total\n\nprint(f\"Total annotations: {n_annotations_total.item()}\")\nprint(f\"Annotations in region: {n_annotations_in_region.item()}\")\nprint(f\"Fraction of annotations in region: {fraction_in_region * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that just over 50% of the annotations are within the region of\ninterest defined by the polygon.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform dataset to a ``movement``-like dataset\nWe can take further advantage of ``movement`` utilities by transforming\nour annotations dataset to a ``movement``-like dataset.\n\nTo do this, we need to rename the dataset dimensions,\nadd a confidence array, and add a ``time_unit`` attribute.\nWe additionally rename the\n``individuals`` coordinate values to follow the ``movement``\nnaming convention.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Rename dimensions\nds_as_movement = ds.rename({\"image_id\": \"time\", \"id\": \"individuals\"})\n\n# Rename 'individuals' coordinate values to be\nds_as_movement[\"individuals\"] = [\n    f\"id_{i.item()}\" for i in ds_as_movement.individuals.values\n]\n\n# Add confidence array with NaN values\nds_as_movement[\"confidence\"] = xr.DataArray(\n    np.full(\n        (\n            ds_as_movement.sizes[\"time\"],\n            ds_as_movement.sizes[\"individuals\"],\n        ),\n        np.nan,\n    ),\n    dims=[\"time\", \"individuals\"],\n)\n\n# Add time_unit attribute\nds_as_movement.attrs[\"time_unit\"] = \"frames\"\n\n\nprint(ds_as_movement)\nprint(ds_as_movement.sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this dataset represents manually labelled data, there isn't\nreally a confidence value associated with each of the annotations.\nTherefore, we add a confidence array with NaN values.\n\nSimilarly, we set the time unit to ``frames``, but actually the images do not\nrepresent consecutive images in time. We do this to later be able to\nexport the dataset in a ``movement``-supported format that we can\nvisualise in the [movement napari plugin](https://movement.neuroinformatics.dev/user_guide/gui.html)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot occupancy map using ``movement``\n\nWe can now use the :func:`movement.plots.plot_occupancy` function to plot\nthe occupancy map of the annotations. This is a two-dimensional histogram\nthat shows for each bin the number of annotations that fall within it.\n\nTo determine the number of bins along each dimension, we use the aspect ratio\nof the images to define similarly sized bins.\nThis makes the occupancy map more informative. Note that all images have the\nsame dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Determine aspect ratio of the images\nimage_width = np.unique(ds[\"image_shape\"].sel(space=\"x\").values).item()\nimage_height = np.unique(ds[\"image_shape\"].sel(space=\"y\").values).item()\nimage_AR = image_width / image_height\n\n# Set number of bins along each dimension\nn_bins_x = 75\nn_bins_y = int(n_bins_x / image_AR)\n\n# Plot occupancy map\nfig, ax, hist = plot_occupancy(\n    ds_as_movement.position,\n    bins=[n_bins_x, n_bins_y],\n)\nfig.set_size_inches(10, 5)\nax.set_xlim(0, image_width)\nax.set_ylim(0, image_height)\nax.set_xlabel(\"x (pixels)\")\nax.set_ylabel(\"y (pixels)\")\nax.axis(\"equal\")\nax.invert_yaxis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The occupancy map shows that the maximum count in each bin is 5 annotations,\nand the minimum count is 0 annotations. We can confirm this by inspecting\nthe outputs of the :func:`movement.plots.plot_occupancy` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bin_size_x = np.diff(hist[\"xedges\"])[0].item()\nbin_size_y = np.diff(hist[\"yedges\"])[0].item()\n\nprint(f\"Bin size (pixels): ({bin_size_x}, {bin_size_y})\")\nprint(f\"Maximum bin count: {hist['counts'].max().item()}\")\nprint(f\"Minimum bin count: {hist['counts'].min().item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualise the dataset in the ``movement`` napari plugin\nWe can export the ``movement``-like dataset in a format\nthat we can visualise in the [movement napari plugin](https://movement.neuroinformatics.dev/user_guide/gui.html).\nFor example, we can use the\n:func:`movement.io.save_bboxes.to_via_tracks_file` function, that saves\nbounding box ``movement`` datasets as VIA-tracks files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "save_bboxes.to_via_tracks_file(ds_as_movement, \"waterfowl_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now follow the [movement napari guide](https://movement.neuroinformatics.dev/user_guide/gui.html)\nto load the output VIA-tracks file into ``napari``.\n\nTo visualise the annotations over the corresponding images, remember to\nfirst drag and drop the images directory into the ``napari`` canvas.\nYou will find the images for the experts' annotations under the\n``data_dir / \"experts\" / \"images\"`` directory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Images directory: {data_dir / 'experts' / 'images'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The view in ``napari`` should look something like this:\n\n<img src=\"file://../_static/examples/napari-annotations.jpg\" alt=\"Bounding box annotations in napari\">\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The bounding boxes are coloured by individual ID per image.\nRemember that the individual IDs are not consistent across images, so it\nmakes more sense to hide the tracks layer for an easier visualisation,\nlike in the example screenshot above.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean-up\nTo remove the output files we have just created, we can run the\nfollowing code.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "os.remove(\"waterfowl_dataset.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}